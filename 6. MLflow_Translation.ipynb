{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Translation with Transformers and MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=false\n"
     ]
    }
   ],
   "source": [
    "# Disable tokenizers warnings when constructing pipelines\n",
    "%env TOKENIZERS_PARALLELISM=false\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Disable a few less-than-useful UserWarnings from setuptools and pydantic\n",
    "warnings.filterwarnings(\"ignore\", category = UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import mlflow\n",
    "\n",
    "model_architecture = \"google/flan-t5-base\"\n",
    "\n",
    "translation_pipeline = transformers.pipeline(\n",
    "    task = \"translation_en_to_fr\",\n",
    "    model = transformers.T5ForConditionalGeneration.from_pretrained(\n",
    "        model_architecture, max_length = 1000\n",
    "    ),\n",
    "    tokenizer = transformers.T5TokenizerFast.from_pretrained(model_architecture, return_tensors = 'pt')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Translation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': \"J'ai apprécié mon sajour lente sur les Champs-Élysées.\"}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the pipeline on a sample sentence prior to logging\n",
    "translation_pipeline(\n",
    "    \"translate English to French: I enjoyed my slow saunter along the Champs-Élysées.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Model Parameters and Inferring Signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters that we are permitting to be used at inference time\n",
    "import mlflow.models\n",
    "\n",
    "model_params = {'max_length': 1000}\n",
    "\n",
    "# Generate the model signature by providing an input the expected output\n",
    "signature = mlflow.models.infer_signature(\n",
    "    \"This is a sample input sentence.\",\n",
    "    mlflow.transformers.generate_signature_output(translation_pipeline, \"This is another sample.\"),\n",
    "    params = model_params\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reviewing the Model Signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inputs: \n",
       "  [string (required)]\n",
       "outputs: \n",
       "  [string (required)]\n",
       "params: \n",
       "  ['max_length': integer (default: 1000)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize the model signature\n",
    "signature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///e:/MLFlow/mlruns/410751203367765635', creation_time=1731914900050, experiment_id='410751203367765635', last_update_time=1731914900050, lifecycle_stage='active', name='Translation', tags={}>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_experiment(\"Translation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging the Model with MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 300, 'early_stopping': True, 'num_beams': 4}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\25869\\AppData\\Local\\miniconda2\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/18 14:32:54 WARNING mlflow.utils.environment: On Windows, timeout is not supported for model requirement inference. Therefore, the operation is not bound by a timeout and may hang indefinitely. If it hangs, please consider specifying the signature manually.\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run():\n",
    "    model_info = mlflow.transformers.log_model(\n",
    "        transformers_model = translation_pipeline,\n",
    "        artifact_path = \"french_translator\",\n",
    "        signature = signature,\n",
    "        model_params = model_params, \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensuring Component Integrity and Functionality \n",
    "Inspecting these components ensures that:\n",
    "* The model aligns with our task requirements.\n",
    "* Hardware resources are optimally utilized.\n",
    "* Text inputs are correctly preprocessed for model consumption.\n",
    "* The model’s compatibility with the selected deep learning framework is confirmed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/18 14:33:04 INFO mlflow.transformers: 'runs:/305267d7333e4be28692f096be002c6d/french_translator' resolved as 'file:///e:/MLFlow/mlruns/410751203367765635/305267d7333e4be28692f096be002c6d/artifacts/french_translator'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edf90b1897934dee9c56b221044894d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task -> str\n",
      "framework -> str\n",
      "torch_dtype -> dtype\n",
      "model -> T5ForConditionalGeneration\n",
      "tokenizer -> T5TokenizerFast\n"
     ]
    }
   ],
   "source": [
    "# Load our saved model as a dictionary of components, comprising the model\n",
    "translation_components = mlflow.transformers.load_model(\n",
    "    model_info.model_uri, return_type = 'components'\n",
    ")\n",
    "\n",
    "for key, value in translation_components.items():\n",
    "    print(f\"{key} -> {type(value).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Model Flavors in MLflow\n",
    "The model_info.flavors attribute in MLflow provides insights into the model’s capabilities and deployment requirements across various platforms.\n",
    "\n",
    "Flavors in MLflow represent different ways the model can be utilized and deployed. Key aspects include:\n",
    "\n",
    "* Python Function Flavor: Indicates the model’s compatibility as a generic Python function, including model binary, loader module, Python version, and environment specifications.\n",
    "\n",
    "* Transformers Flavor: Tailored for models from the Hugging Face Transformers library, covering transformers version, code dependencies, task, instance type, source model name, pipeline model type, framework, tokenizer type, components, and model binary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'transformers': {'transformers_version': '4.41.2',\n",
       "  'code': None,\n",
       "  'task': 'translation_en_to_fr',\n",
       "  'instance_type': 'TranslationPipeline',\n",
       "  'framework': 'pt',\n",
       "  'torch_dtype': 'torch.float32',\n",
       "  'pipeline_model_type': 'T5ForConditionalGeneration',\n",
       "  'source_model_name': 'google/flan-t5-base',\n",
       "  'model_binary': 'model',\n",
       "  'tokenizer_type': 'T5TokenizerFast',\n",
       "  'components': ['tokenizer']},\n",
       " 'python_function': {'loader_module': 'mlflow.transformers',\n",
       "  'python_version': '3.12.2',\n",
       "  'env': {'conda': 'conda.yaml', 'virtualenv': 'python_env.yaml'}}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_info.flavors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Translation Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/18 14:33:05 INFO mlflow.transformers: 'runs:/305267d7333e4be28692f096be002c6d/french_translator' resolved as 'file:///e:/MLFlow/mlruns/410751203367765635/305267d7333e4be28692f096be002c6d/artifacts/french_translator'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42632b96518e448fbe26055a460b23f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'translation_text': \"J'ai entendu que Nice est bien cette période de l'année.\"}]\n"
     ]
    }
   ],
   "source": [
    "translation_pipeline = mlflow.transformers.load_model(model_info.model_uri)\n",
    "response = translation_pipeline(\"I have heard that Nice is nice this time of year.\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing the Reconstructed Pipeline's Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'translation_text': \"Transformers simplifie l'utilisation des modèles de l'apprentissage profonde!\"}]\n"
     ]
    }
   ],
   "source": [
    "reconstructed_pipeline = transformers.pipeline(**translation_components)\n",
    "\n",
    "reconstructed_response = reconstructed_pipeline(\n",
    "    \"Transformers makes using Deep Learning models easy and fun!\"\n",
    ")\n",
    "\n",
    "print(reconstructed_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct Utilization of Model Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['task', 'framework', 'torch_dtype', 'model', 'tokenizer'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation_components.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Usage: Direct Interaction with Model Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "La liberté, l'égalité, la fraternité ou la mort.\n"
     ]
    }
   ],
   "source": [
    "# Access the individual components from the components dictionary\n",
    "tokenizer = translation_components[\"tokenizer\"]\n",
    "model = translation_components[\"model\"]\n",
    "\n",
    "query = \"Translate to French: Liberty, equality, fraternity, or death.\"\n",
    "\n",
    "# This notebook was run on a Mac laptop, so we'll send the output tensor to the \"mps\" device.\n",
    "# If you're running this on a different system, ensure that you're sending the tensor output to the appropriate device to ensure that\n",
    "# the model is able to read it from memory.\n",
    "inputs = tokenizer.encode(query, return_tensors=\"pt\").to(\"cpu\")\n",
    "outputs = model.generate(inputs).to(\"cpu\")\n",
    "result = tokenizer.decode(outputs[0])\n",
    "\n",
    "# Since we're not using a pipeline here, we need to modify the output slightly to get only the translated text.\n",
    "print(result.replace(\"<pad> \", \"\\n\").replace(\"</s>\", \"\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
