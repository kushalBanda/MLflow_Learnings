{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Conversational AI with MLflow and DialoGPT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=false\n"
     ]
    }
   ],
   "source": [
    "# Disable tokenizers warnings when constructing pipelines\n",
    "%env TOKENIZERS_PARALLELISM=false\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Disable a few less-than-useful UserWarnings from setuptools and pydantic\n",
    "warnings.filterwarnings(\"ignore\", category = UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up th Conversational Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "import mlflow.models\n",
    "import mlflow.models\n",
    "import transformers\n",
    "import mlflow \n",
    "\n",
    "# Define our pipeline \n",
    "conversational_pipeline = transformers.pipeline(model = \"microsoft/DialoGPT-medium\")\n",
    "\n",
    "# Infer the signature\n",
    "signature = mlflow.models.infer_signature(\n",
    "    \"Hi there, Chatbot!\",\n",
    "    mlflow.transformers.generate_signature_output(conversational_pipeline, \"Hi there, Another example\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an Experiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/18 15:28:35 INFO mlflow.tracking.fluent: Experiment with name 'Conversational' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///e:/MLFlow/mlruns/186907240044108028', creation_time=1731923915866, experiment_id='186907240044108028', last_update_time=1731923915866, lifecycle_stage='active', name='Conversational', tags={}>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_experiment(\"Conversational\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set our tracking uri for logging\n",
    "mlflow.set_tracking_uri(uri = \"http://127.0.0.1:8080\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging the Model with MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/18 15:43:16 WARNING mlflow.utils.environment: On Windows, timeout is not supported for model requirement inference. Therefore, the operation is not bound by a timeout and may hang indefinitely. If it hangs, please consider specifying the signature manually.\n",
      "2024/11/18 15:43:24 INFO mlflow.tracking._tracking_service.client: üèÉ View run intrigued-conch-438 at: http://127.0.0.1:8080/#/experiments/186907240044108028/runs/055adab0f56e4fbd8335ce8c79b717ee.\n",
      "2024/11/18 15:43:24 INFO mlflow.tracking._tracking_service.client: üß™ View experiment at: http://127.0.0.1:8080/#/experiments/186907240044108028.\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run():\n",
    "    model_info = mlflow.transformers.log_model(\n",
    "        transformers_model=conversational_pipeline,\n",
    "        artifact_path=\"chatbot\",\n",
    "        task=\"conversational\",\n",
    "        signature=signature\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Interacting with the Chatbot Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation id: f66ce8d9-1be9-492f-b6ed-50392c2f46c4\n",
      "user: What is the best way to get to Antarctica?\n",
      "assistant: I think you can get there by boat.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import Conversation\n",
    "\n",
    "# Create a conversational pipeline\n",
    "chat_pipeline = pipeline(\"conversational\", model = \"microsoft/DialoGPT-medium\")\n",
    "\n",
    "conversation = Conversation(\"What is the best way to get to Antarctica?\")\n",
    "response = chat_pipeline([conversation])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3e6b80d28614d1b8b2377e557fcf7b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# Load the model as a generic python function in order to leverage the integrated Conversational Context\n",
    "# Note that loading a conversational model with the native flavor (i.e., `mlflow.transformers.load_model()`) will not include anything apart from the\n",
    "# pipeline itself; if choosing to load in this way, you will need to manage your own Conversational Context instance to maintain state on the\n",
    "# conversation history.\n",
    "chatbot = mlflow.pyfunc.load_model(model_uri=model_info.model_uri)\n",
    "\n",
    "# Validate that the model is capable of responding to a question\n",
    "first = chatbot.predict(\"What is the best way to get to Antarctica?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: ['What is the best way to get to Antarctica?']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Response: {first}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# Verify that the PyFunc implementation has maintained state on the conversation history by asking a vague follow-up question that requires context\n",
    "# in order to answer properly\n",
    "second = chatbot.predict(\"What sort of boat should I use?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: ['What sort of boat should I use?']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Response: {second}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
