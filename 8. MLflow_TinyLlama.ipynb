{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and Serving an OpenAI compatiable Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\25869\\AppData\\Local\\miniconda2\\Lib\\site-packages\\~okenizers'.\n",
      "  You can safely remove it manually.\n"
     ]
    }
   ],
   "source": [
    "!pip install mlflow>=2.11.0 -q -U\n",
    "# OpenAI-compatible chat model support is available for Transformers 4.34.0 and above\n",
    "!pip install transformers>=4.34.0 -q -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=false\n"
     ]
    }
   ],
   "source": [
    "# Disable tokenizers warnings when constructing pipelines\n",
    "%env TOKENIZERS_PARALLELISM=false\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Disable a few less-than-useful UserWarnings from setuptools and pydantic\n",
    "warnings.filterwarnings(\"ignore\", category = UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Chat Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fbcf92820d84b489895362817938741",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81648f88d54d4e0fb980c324ca117c9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4feabc5a1ddb4464b8056c0ef38a9310",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de666f08ed1e450a87178c231593a9cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59cfdea740654740b70e74f458f3c7d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "108c2bd1dde2494a8a45a236b7ea9737",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d660f325262848ccb7704902a5e991a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73aeb9545d82434bb0ef21f3f7518b59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/3.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\25869\\AppData\\Local\\miniconda2\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/18 16:19:55 WARNING mlflow.utils.environment: On Windows, timeout is not supported for model requirement inference. Therefore, the operation is not bound by a timeout and may hang indefinitely. If it hangs, please consider specifying the signature manually.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "import mlflow\n",
    "\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    ")\n",
    "\n",
    "# save the model using the vanilla `text-generation` task type\n",
    "mlflow.transformers.save_model(\n",
    "    path = \"tinyllama-text-generation\", transformers_model = generator, task = \"text-generation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e4e3894b09b4c02ad208d9d89221715",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "inputs: \n",
       "  [string (required)]\n",
       "outputs: \n",
       "  [string (required)]\n",
       "params: \n",
       "  None"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model for inference \n",
    "model = mlflow.pyfunc.load_model('tinyllama-text-generation')\n",
    "\n",
    "model.metadata.signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|user|>\\nWrite me a hello world program in python</s>\\n<|assistant|>\\nHere\\'s a simple hello world program in Python:\\n\\n```python\\nprint(\"Hello, world!\")\\n```\\n\\nThis program prints the string \"Hello, world!\" to the console.\\n\\nTo run this program, simply run it from the command line or a text editor. For example:\\n\\n```\\npython\\n>>> print(\"Hello, world!\")\\nHello, world!\\n```']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first, apply the tokenizer's chat template, since the\n",
    "# model is tuned to accept prompts in a chat format. this\n",
    "# also converts the list of messages to a string.\n",
    "messages = [{\"role\": \"user\", \"content\": \"Write me a hello world program in python\"}]\n",
    "prompt = generator.tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "\n",
    "model.predict(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/18 17:21:18 WARNING mlflow.utils.environment: On Windows, timeout is not supported for model requirement inference. Therefore, the operation is not bound by a timeout and may hang indefinitely. If it hangs, please consider specifying the signature manually.\n"
     ]
    }
   ],
   "source": [
    "mlflow.transformers.save_model(\n",
    "    path = \"tinyllama-chat\", transformers_model = generator, task = \"llm/v1/chat\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d739f67dcc1b4b83a8f723c8eb331726",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "inputs: \n",
       "  ['messages': Array({content: string (optional), name: string (optional), refusal: string (optional), role: string (required), tool_call_id: string (optional), tool_calls: Array({function: {arguments: string (required), name: string (required)} (required), id: string (required), type: string (required)}) (optional)}) (required), 'temperature': double (optional), 'max_tokens': long (optional), 'stop': Array(string) (optional), 'n': long (optional), 'stream': boolean (optional), 'top_p': double (optional), 'top_k': long (optional), 'frequency_penalty': double (optional), 'presence_penalty': double (optional), 'tools': Array({function: {description: string (optional), name: string (required), parameters: {additionalProperties: boolean (optional), properties: Map(str -> {description: string (optional), enum: Array(string) (optional), items: {type: string (required)} (optional), type: string (required)}) (required), required: Array(string) (optional), type: string (optional)} (required), strict: boolean (optional)} (optional), type: string (required)}) (optional), 'metadata': Map(str -> DataType.string) (optional)]\n",
       "outputs: \n",
       "  ['id': string (required), 'object': string (required), 'created': long (required), 'model': string (required), 'choices': Array({finish_reason: string (required), index: long (required), message: {content: string (optional), name: string (optional), refusal: string (optional), role: string (required), tool_call_id: string (optional), tool_calls: Array({function: {arguments: string (required), name: string (required)} (required), id: string (required), type: string (required)}) (optional)} (required)}) (required), 'usage': {completion_tokens: long (required), prompt_tokens: long (required), total_tokens: long (required)} (optional), 'metadata': Map(str -> DataType.string) (optional)]\n",
       "params: \n",
       "  None"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = mlflow.pyfunc.load_model(\"tinyllama-chat\")\n",
    "model.metadata.signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '2b3a58fa-5dc9-4c30-ab82-6fd2d43f1c89',\n",
       "  'object': 'chat.completion',\n",
       "  'created': 1731930788,\n",
       "  'model': 'TinyLlama/TinyLlama-1.1B-Chat-v1.0',\n",
       "  'usage': {'prompt_tokens': 24, 'completion_tokens': 87, 'total_tokens': 111},\n",
       "  'choices': [{'index': 0,\n",
       "    'finish_reason': 'stop',\n",
       "    'message': {'role': 'assistant',\n",
       "     'content': 'Here\\'s a simple hello world program in Python:\\n\\n```python\\nprint(\"Hello, world!\")\\n```\\n\\nThis program prints the string \"Hello, world!\" to the console.\\n\\nTo run this program, simply run it from the command line or a text editor. For example:\\n\\n```\\npython\\n>>> print(\"Hello, world!\")\\nHello, world!\\n```'}}]}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [{'role': 'user', \"content\": \"Write me a hello world program in python\"}]\n",
    "\n",
    "model.predict({\"messages\": messages})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
